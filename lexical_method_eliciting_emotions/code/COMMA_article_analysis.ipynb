{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMMA_article_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kAkHy0V9OJn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option(\"max_colwidth\", 400)\n",
        "pd.set_option(\"display.max.columns\", None)\n",
        "pd.set_option(\"display.max.rows\", None)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.style.use(\"seaborn-talk\")\n",
        "import matplotlib.ticker as mtick\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "np.seterr(divide='ignore')\n",
        "warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
        "pd.options.mode.chained_assignment = None "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0525FxD3BaL"
      },
      "source": [
        "def clean_text(dataframe, text_column):\n",
        "  '''Parameters:\n",
        "  dataframe: dataframe with data,\n",
        "  \n",
        "  text_column: str - name of the column in the dataframe where the text you want to clean is listed\n",
        "  '''\n",
        "  import re\n",
        "  import string\n",
        "  df = dataframe.copy()\n",
        "  all_texts = []\n",
        "  for text in df[text_column]:\n",
        "    text = re.sub(r\"(http|https):\\/\\/([\\w\\s\\d\\.]+)(\\/?)(.*)\", \" \", str(text).lower()) #  urls\n",
        "    text = re.sub(r\"(www).([\\w\\s\\d\\.]+)(\\/?)(.*)\", \" \", text) #  urls\n",
        "    text = re.sub('@[\\w\\d]+',' ', text)  # mentions\n",
        "    text = text.replace(\"\\n\", \" \") # new lines\n",
        "    text = re.sub(r'\\B#\\w*[a-zA-Z0-9]+\\w*',' ', text) # hashtags\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s\\s+', ' ', text)\n",
        "    all_texts.append(text)\n",
        "  df[\"clean_\" + text_column] = all_texts\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(dataframe, text_column):\n",
        "  '''Parameters:\n",
        "  dataframe: dataframe with your data,\n",
        "  \n",
        "  text_column: column of a dataframe where text is located\n",
        "  '''\n",
        "  df = dataframe.copy()\n",
        "  lemmas = []\n",
        "  for doc in nlp.pipe(df[text_column].apply(str)):\n",
        "    lemmas.append([token.lemma_ for token in doc if (not token.is_punct and len(token) > 1)])\n",
        "  df[text_column+\"_lemmatized\"] = lemmas\n",
        "  return df"
      ],
      "metadata": {
        "id": "aBmiO49SHSr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pathos_inducers(dataframe, content_lemmatized_column, affective_database_path, db_words = \"Word\", uniq_words=False):\n",
        "  '''Parameters: \n",
        "  dataframe: dataframe with your data,\n",
        "\n",
        "  content_lemmatized_column: str - name of a column in dataframe where lemmatized text is located,\n",
        "  \n",
        "  affective_database_path: str - path to a file with affective database,\n",
        "  \n",
        "  db_words: str - name of a column in affective database where words are listed,\n",
        "  \n",
        "  uniq_words: boolean - True if you want to retrieve only unique emotive words from your text data,\n",
        "  False if you want to retrieve every emotive word (thus, there can be duplicated words),\n",
        "  --> *by default it is set to False\n",
        "  '''\n",
        "\n",
        "  if affective_database_path.endswith(\".xlsx\"):\n",
        "    affective_database = pd.read_excel(affective_database_path)\n",
        "  elif affective_database_path.endswith(\".csv\"):\n",
        "    affective_database = pd.read_csv(affective_database_path)\n",
        "\n",
        "  affective_database = affective_database[[db_words]]\n",
        "  affective_database_emotive_words = affective_database[db_words].tolist()\n",
        "\n",
        "  all_emotive_words = []\n",
        "  if uniq_words == True:\n",
        "    for lemmas_list in dataframe[content_lemmatized_column]:\n",
        "      emotive_words = [word for word in set(lemmas_list).intersection(affective_database[db_words])]\n",
        "      all_emotive_words.append(emotive_words)\n",
        "\n",
        "  elif uniq_words == False:\n",
        "    for lemmas_list in dataframe[content_lemmatized_column]:\n",
        "      emotive_words = []\n",
        "      for word in lemmas_list:\n",
        "        if word in affective_database_emotive_words:\n",
        "          emotive_words.append(word)\n",
        "\n",
        "      all_emotive_words.append(emotive_words)\n",
        "  \n",
        "  dataframe[content_lemmatized_column[:-10]+\"pathos_inducers\"] = all_emotive_words\n",
        "  return dataframe"
      ],
      "metadata": {
        "id": "n-YQP1dtHivv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_polarity_score(dataframe, content_lemmatized_column, affective_database_path, db_words = \"Word\"):\n",
        "  '''Parameters: \n",
        "  dataframe: dataframe with your data,\n",
        "\n",
        "  content_lemmatized_column: str - name of a column in dataframe where words-lemmas are listed\n",
        "  \n",
        "  affective_database_path: str - path to a file with affective database,\n",
        "  \n",
        "  db_words: str - name of a column in affective database where words are listed\n",
        "  '''\n",
        "  affective_database = load_data(affective_database_path)\n",
        "\n",
        "  emotion_values = [\"Valence_standardized\"]\n",
        "  used_cols = [db_words] + emotion_values\n",
        "\n",
        "  affective_database_polarity = affective_database[used_cols]\n",
        "  affective_database_polarity.set_index(db_words, inplace=True)\n",
        "\n",
        "\n",
        "  all_neg_percent = []\n",
        "  all_pos_percent = []\n",
        "\n",
        "  affective_database_polarity_words = affective_database[db_words].tolist()\n",
        "\n",
        "  for lemmas_list in dataframe[content_lemmatized_column]:\n",
        "    emotive_words = []\n",
        "    for word in lemmas_list:\n",
        "      if word in affective_database_polarity_words:\n",
        "        emotive_words.append(word)\n",
        "    \n",
        "    if len(emotive_words) > 0:\n",
        "      scores = affective_database_polarity.loc[emotive_words]\n",
        "\n",
        "      neg_scores_count = scores.where(scores[\"Valence_standardized\"] < -0.5).count()[0]\n",
        "\n",
        "      pos_scores_count = scores.where(scores[\"Valence_standardized\"] > 1).count()[0]\n",
        "\n",
        "      neg_percent = round((neg_scores_count / len(lemmas_list)), 3)\n",
        "      all_neg_percent.append(neg_percent)\n",
        "\n",
        "      pos_percent = round((pos_scores_count / len(lemmas_list)), 3)\n",
        "      all_pos_percent.append(pos_percent)\n",
        "      \n",
        "    else:\n",
        "      neg_percent=pos_percent = np.NaN \n",
        "      all_neg_percent.append(neg_percent)\n",
        "      all_pos_percent.append(pos_percent)\n",
        "\n",
        "  dataframe[content_lemmatized_column[:-10]+\"Negative_percentage\"] = all_neg_percent\n",
        "  dataframe[content_lemmatized_column[:-10]+\"Positive_percentage\"] = all_pos_percent\n",
        "\n",
        "  return dataframe"
      ],
      "metadata": {
        "id": "qOJFYekJHvMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess data"
      ],
      "metadata": {
        "id": "kOWMwE3mIDaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eliciting valence data\n",
        "data_debate = load_data(\"/content/drive/MyDrive/Colab Notebooks/debates/debate_2020_June.xlsx\")\n",
        "data_debate.head(1)"
      ],
      "metadata": {
        "id": "rivwppZmIGce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in [\"conclusion\", \"premise\", \"full_argument\"]:\n",
        "  print(col)\n",
        "  data = lemmatization(df, col) \n",
        "\n",
        "  data = find_pathos_inducers(data, content_lemmatized_column = col+'_lemmatized', \n",
        "                                           affective_database_path = \"/content/drive/MyDrive/Colab Notebooks/Emotional word lists/joined_scaled_filled_0_NAWL-Sentimenti_db.xlsx\", \n",
        "                                           db_words = \"Word\")\n",
        "\n",
        "  data = get_polarity_score(data, content_lemmatized_column = col, \n",
        "                        affective_database_path = \"/content/drive/MyDrive/Colab Notebooks/Emotional word lists/valence_only10k_scaled_NAWL-Sentimenti_Imbir.xlsx\")\n"
      ],
      "metadata": {
        "id": "5Ppmfys4IFcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expressed sentiment data\n",
        "\n",
        "social_media = load_data(\"/content/drive/MyDrive/Colab Notebooks/debates/validation_samples/tweet_Czerwiec_expressed_sentiment.xlsx\")\n",
        "social_media.sort_values(by = 'Data', inplace=True)\n",
        "social_media = social_media[social_media.Data < '2020-06-17 23:00:00']\n",
        "social_media = clean_text(social_media, \"Tekst\")\n",
        "social_media.shape"
      ],
      "metadata": {
        "id": "bLAZhjUkJWa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Filter SM data"
      ],
      "metadata": {
        "id": "ojo7nooKJlUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_candidates(social_media, candidate = 'Andrzej Duda'):\n",
        "  \"\"\"\n",
        "  'Andrzej Duda', \n",
        "  'Rafał Trzaskowski'\n",
        "\n",
        "  \"\"\"\n",
        "  key_words_duda = ['duda', 'andrzej', 'pad ', 'dudy', 'dudzie', 'anżej', 'rzad', 'rząd']\n",
        "  key_words_trzask = ['rafa', 'trzask', 'warszaw', 'platform', ' ko ', 'kidaw']\n",
        "\n",
        "  if candidate == 'Andrzej Duda':\n",
        "    key_words = key_words_duda\n",
        "  elif candidate == 'Rafał Trzaskowski':\n",
        "    key_words = key_words_trzask\n",
        "\n",
        "  sm2 = social_media[['Data', 'Tekst', 'clean_Tekst',\n",
        "                      'sentiment_tuned_PaRes', 'sentiment_label_PaRes']]\n",
        "  \n",
        "  sm2 = sm2.reset_index(drop=True)\n",
        "  sm2['Tekst'] = sm2.Tekst.apply(lambda x: str(x).lower())\n",
        "  k_ids = []\n",
        "  for k in key_words:\n",
        "    for i in sm2.index:\n",
        "      if k in sm2.loc[i, 'Tekst']:\n",
        "        k_ids.append(i)  \n",
        "\n",
        "  sm_filtered = sm2.loc[k_ids]\n",
        "  sm_filtered.drop_duplicates(\"Tekst\", inplace=True)\n",
        "  sm_filtered.sort_values(by = \"Data\", inplace=True)\n",
        "  print(f\"Found: {len(sm_filtered)} tweets for candidate: {candidate}\")\n",
        "  return sm_filtered"
      ],
      "metadata": {
        "id": "Ck1WMEjhJo5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_data(dataframe_social_media, dataframe_debate):\n",
        "  \"\"\"\n",
        "  Parameters:  \n",
        "\n",
        "  dataframe_social_media: data with social media reactions (comments), \n",
        "\n",
        "  dataframe_debate:  data with debate (politicians' arguments)\n",
        "\n",
        "  \"\"\"\n",
        "  sm_data = dataframe_social_media.copy()\n",
        "  deb_data = dataframe_debate.copy()\n",
        "\n",
        "  sm_data.sort_values(by = 'Data', inplace=True)\n",
        "\n",
        "  # eliciting emotions\n",
        "  deb_data[\"Time\"] = pd.to_datetime(deb_data.start)\n",
        "  deb_data.sort_values(by = \"Time\", inplace=True)\n",
        "  df_plot = deb_data.set_index(\"Time\").resample(\"1T\").mean().fillna(0)\n",
        "  df_plot.reset_index(inplace=True)\n",
        "  df_plot[\"Time\"] = df_plot[\"Time\"].dt.time.apply(str)\n",
        "\n",
        "  df_plots_counts_normalized = pd.DataFrame(sm_data.set_index(\"Data\").shift(periods=-1, \n",
        "                                                                              freq=\"T\", \n",
        "                                                                              axis=0).resample(\"1T\")[\"sentiment_label_PaRes\"].value_counts(normalize=True)) * 100\n",
        "  df_plots_counts_normalized.columns = ['_'.join(col) for col in df_plots_counts_normalized.columns]\n",
        "  df_plots_counts_normalized = df_plots_counts_normalized.reset_index()\n",
        "  df_plots_counts_normalized.columns = ['Data', 'sentiment_label_PaRes', 'mean']\n",
        "\n",
        "  df_plots_counts_normalized.sort_values(by = ['Data', 'sentiment_label_PaRes'], inplace=True)\n",
        "  df_plots_counts_normalized[\"Time\"] = df_plots_counts_normalized[\"Data\"].dt.time.apply(str)\n",
        "    \n",
        "  # merge social media and debate data\n",
        "  df_plot_join = pd.merge(df_plots_counts_normalized, df_plot, on = \"Time\", how = \"left\")\n",
        "    \n",
        "  cols2 = ['full_argument_Negative_percentage','full_argument_Positive_percentage']\n",
        "  df_plot_join = df_plot_join[['Data', 'mean', 'sentiment_label_PaRes', 'Time']+cols2]\n",
        "\n",
        "  # elicited valence \n",
        "  df_baselines_diff = df_plot_join.copy()\n",
        "    \n",
        "  # expressed sentiment baselines\n",
        "  expressed_senti_baselines = pd.DataFrame(df_plot_join.groupby(\"sentiment_label_PaRes\")[\"mean\"].mean()).reset_index()\n",
        "  expressed_senti_baselines = expressed_senti_baselines.set_index(\"sentiment_label_PaRes\").T\n",
        "  for col in set(expressed_senti_baselines.columns):\n",
        "    ids = df_baselines_diff[df_baselines_diff.sentiment_label_PaRes == col][\"mean\"].index\n",
        "    df_baselines_diff.loc[ids, \"mean\"] = df_baselines_diff.loc[ids, \"mean\"] - expressed_senti_baselines[col].iloc[0]\n",
        "\n",
        "  return df_baselines_diff"
      ],
      "metadata": {
        "id": "s9ZetzraK4Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AD"
      ],
      "metadata": {
        "id": "H2Hpyb-tNBm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm_duda = filter_candidates(social_media = social_media)\n",
        "\n",
        "duda_df_base = normalise_data(dataframe_social_media = sm_duda, \n",
        "                                        dataframe_debate = data_debate)\n",
        "duda_df_base['full_argument_valence_score'] = duda_df_base['full_argument_Positive_percentage'] - duda_df_base['full_argument_Negative_percentage']\n",
        "print(duda_df_base.shape, '\\n')\n",
        "\n",
        "\n",
        "deb_duda = data_debate[data_debate.speaker == 'Andrzej Duda']\n",
        "print(deb_duda.shape, '\\n')\n",
        "duda_utterance_time = deb_duda.start.unique()\n",
        "\n",
        "duda_utterance_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8_CmXU1Wx5s",
        "outputId": "3ce843fe-cd33-4b8d-cb8e-f53f00450f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['21:04:00', '21:22:00', '22:06:00', '21:42:00'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deb_duda_scatter = duda_df_base[duda_df_base.Time.isin(duda_utterance_time)]"
      ],
      "metadata": {
        "id": "bxVD3MlNKVC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot_neg = duda_df_base[duda_df_base.sentiment_label_PaRes == \"neg\"]\n",
        "df_plot_pos = duda_df_base[duda_df_base.sentiment_label_PaRes == \"pos\"]\n",
        "\n",
        "deb_duda_scatter2 = deb_duda_scatter[deb_duda_scatter.sentiment_label_PaRes=='neg']\n",
        "\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.style.use(\"seaborn-talk\")\n",
        "\n",
        "fig, ax1 = plt.subplots(1, 1, figsize=(15, 8.5))\n",
        "x = list(df_plot_neg.Time)\n",
        "\n",
        "ax1.plot(df_plot_neg[\"Time\"], df_plot_neg[\"full_argument_valence_score\"]*100, label = \"valence\", \n",
        "         color = \"#525252\", alpha=0.85, linewidth = 2.6)\n",
        "\n",
        "\n",
        "plt.scatter(deb_duda_scatter2[\"Time\"], deb_duda_scatter2[\"full_argument_valence_score\"]*100, \n",
        "            color = \"#0900A4\", label = \"Andrzej Duda arguments\", alpha = 0.9, s = 120)\n",
        "xx = deb_duda_scatter2[\"Time\"].values\n",
        "yy = deb_duda_scatter2[\"full_argument_valence_score\"].values*100\n",
        "for i, txt in enumerate(xx):\n",
        "    ax1.annotate(txt, (xx[i], 27), xycoords=\"data\", \n",
        "                 xytext=(-20, 0), textcoords=\"offset points\",\n",
        "                  va=\"center\", ha=\"left\", color = '#0900A4',\n",
        "                  bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
        "                  arrowprops=dict(arrowstyle=\"->\"))\n",
        "\n",
        "\n",
        "ax1.plot(df_plot_neg[\"Time\"], df_plot_neg[\"mean\"], label = \"negative sentiment expressed\", \n",
        "         color = \"#E90000\", alpha=0.65, linestyle=\"--\", linewidth = 2.5)\n",
        "\n",
        "ax1.plot(df_plot_pos[\"Time\"], df_plot_pos[\"mean\"], label = \"positive sentiment expressed\", \n",
        "         color = \"#00E965\", alpha=0.75, linestyle=\"--\", linewidth = 2.5)\n",
        "\n",
        "ax1.set_xticks(x[::3])\n",
        "ax1.set_xticklabels(x[::3], rotation=90, size=12)\n",
        "ax1.set_xlabel(\"\\nTime\")\n",
        "\n",
        "ax1.set_title(\"Positivity and negativity in 2020 June debate - Andrzej Duda keywords \\n\\n\", fontsize = 15)\n",
        "ax1.set_ylabel(\"Value\\n\")\n",
        "plt.xticks(rotation='vertical', size=11)\n",
        "#plt.yticks(np.arange(-20, 56, 5))\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.08), ncol=6)\n",
        "\n",
        "ax1.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kap833SYM_IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RT"
      ],
      "metadata": {
        "id": "5omd_VGiNDk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm_rt = filter_candidates(social_media = social_media, candidate=\"Rafał Trzaskowski\")\n",
        "\n",
        "rt_df_base = normalise_data(dataframe_social_media = sm_rt, dataframe_debate = data_debate)\n",
        "rt_df_base['full_argument_valence_score'] = rt_df_base['full_argument_Positive_percentage'] - rt_df_base['full_argument_Negative_percentage']\n",
        "print(rt_df_base.shape, '\\n')\n",
        "\n",
        "deb_rt = data_debate[data_debate.speaker == 'Rafał Trzaskowski']\n",
        "print(deb_rt.shape, '\\n')\n",
        "\n",
        "rt_utterance_time = deb_rt.start.unique()\n",
        "\n",
        "rt_utterance_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbTdDTfZXzdT",
        "outputId": "872d77ca-3e73-47b0-e998-6078c9bb529b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['21:55:00', '21:13:00', '22:09:00', '21:45:00', '21:34:00'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deb_rt_scatter = rt_df_base[rt_df_base.Time.isin(rt_utterance_time)]"
      ],
      "metadata": {
        "id": "s_nzkMxgKZEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot_neg = rt_df_base[rt_df_base.sentiment_label_PaRes == \"neg\"]\n",
        "df_plot_pos = rt_df_base[rt_df_base.sentiment_label_PaRes == \"pos\"]\n",
        "\n",
        "deb_rt_scatter2 = deb_rt_scatter[deb_rt_scatter.sentiment_label_PaRes=='neg']\n",
        "\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.style.use(\"seaborn-talk\")\n",
        "\n",
        "fig, ax1 = plt.subplots(1, 1, figsize=(15, 8.5))\n",
        "x = list(df_plot_pos.Time)\n",
        "\n",
        "ax1.plot(df_plot_neg[\"Time\"], df_plot_neg[\"full_argument_valence_score\"]*100, label = \"valence\", \n",
        "         color = \"#525252\", alpha=0.85, linewidth = 2.6)\n",
        "\n",
        "\n",
        "plt.scatter(deb_rt_scatter2[\"Time\"], deb_rt_scatter2[\"full_argument_valence_score\"]*100, \n",
        "            color = \"#0900A4\", label = \"Rafał Trzaskowski arguments\", alpha = 0.9, s = 120)\n",
        "xx = deb_rt_scatter2[\"Time\"].values\n",
        "yy = deb_rt_scatter2[\"full_argument_valence_score\"].values*100\n",
        "for i, txt in enumerate(xx):\n",
        "  ax1.annotate(txt, (xx[i], 33), xycoords=\"data\", \n",
        "                 xytext=(-20, 0), textcoords=\"offset points\",\n",
        "                  va=\"center\", ha=\"left\", color = '#0900A4',\n",
        "                  bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
        "                  arrowprops=dict(arrowstyle=\"->\"))\n",
        "\n",
        "\n",
        "ax1.plot(df_plot_neg[\"Time\"], df_plot_neg[\"mean\"], label = \"negative sentiment expressed\", \n",
        "         color = \"#E90000\", alpha=0.66, linestyle=\"--\", linewidth = 2.5)\n",
        "\n",
        "ax1.plot(df_plot_pos[\"Time\"], df_plot_pos[\"mean\"], label = \"positivie sentiment expressed\", \n",
        "         color = \"#00E965\", alpha=0.7, linestyle=\"--\", linewidth = 2.5)\n",
        "\n",
        "ax1.set_xticks(x[::3])\n",
        "ax1.set_xticklabels(x[::3], rotation=90, size=12)\n",
        "ax1.set_xlabel(\"\\nTime\")\n",
        "\n",
        "ax1.set_title(\"Positivity and negativity in TVP 2020 June debate - Rafał Trzaskowski keywords \\n\\n\", fontsize = 15)\n",
        "ax1.set_ylabel(\"Value\\n\")\n",
        "plt.xticks(rotation='vertical', size=11)\n",
        "#plt.yticks(np.arange(-20, 56, 5))\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.08), ncol=6)\n",
        "\n",
        "ax1.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G9oTZDDtNxzF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
