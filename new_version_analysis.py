# -*- coding: utf-8 -*-
"""more_efficient_analysis.ipynb

Automatically generated by Colaboratory.

analysis pipe:
1 - text cleaning - basics: punctuation marks, , urls, \n (new lines), hashtags, users' mentions

2 - spacy language model

3 - find emotive words

4 - assign categories to those found emotive words

5 - take avg values scores for emotions and individual values for each emotive word

6 - time series analysis --> choosing time range, resample to time units, avg scores for time units

7 - computing baseline values and differences from baselines

8 - ploting results
"""

import pandas as pd
pd.set_option("max_colwidth", 200)
import datetime
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")
plt.style.use("seaborn-talk")

import warnings
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
warnings.simplefilter(action='ignore', category=DeprecationWarning)
np.seterr(divide='ignore')
warnings.filterwarnings(action='ignore', message='Mean of empty slice')
pd.options.mode.chained_assignment = None  # default='warn'


import spacy
# load SPACY model
nlp = spacy.load('pl_core_news_sm')

from timeit import default_timer
from contextlib import contextmanager

@contextmanager
def timer():
    start_time = default_timer()
    try:
        yield
    finally:
        print("Time taken to execute the function:\n--->  %s seconds  <---\n" % (default_timer() - start_time))


def load_data(file_path, indx = True, indx_col = 0):
  '''Parameters:
  file_path: path to your excel or csv file with data,
  indx: boolean - whether there is index column in your file (usually it is the first column) --> *by default it is set to True
  indx_col: int - if your file has an index column, specify column number here --> *by default it is equal to 0 (first column)
  '''
  if indx == True and file_path.endswith(".xlsx"):
    data = pd.read_excel(file_path, index_col = indx_col)
  elif indx == False and file_path.endswith(".xlsx"):
    data = pd.read_excel(file_path)

  elif indx == True and file_path.endswith(".csv"):
    data = pd.read_csv(file_path, index_col = indx_col)
  elif indx == False and file_path.endswith(".csv"):
    data = pd.read_csv(file_path)
  return data

# clean tweeets text from urls, mentions, punctuation marks
def clean_text(dataframe, text_column):
  '''Parameters:
  dataframe: dataframe with data,
  text_column: str - name of the column in the dataframe where the text you want to clean is listed
  '''
  import re
  import string
  df = dataframe.copy()
  all_texts = []
  for text in df[text_column]:
    text = re.sub(r"(http|https):\/\/([\w\s\d\.]+)(\/?)(.*)", "", str(text)) # urls
    text = re.sub('@[\w]+',' ', text)  # mentions
    text = text.replace("\n", " ") # new lines
    text = re.sub(r'\B#\w*[a-zA-Z]+\w*',' ', text) # hashtags
    text = text.translate(str.maketrans(' ', ' ', string.punctuation)) # punctuation marks
    text = text.strip()
    text = re.sub(r'\s\s\s+', ' ', text) # empty spaces
    all_texts.append(text)
  df["clean_" + text_column] = all_texts
  return df


# function to drop NaN (empty) rows from your data
def drop_empty_content(dataframe, content_column):
  '''Parameters:
  dataframe: dataframe with data,

  content_column: str - name of the column in the dataframe based on which you want to drop missing values,
  --> (*hint: it should be cleaned_text column returned from the previous function or just a column containing text data)
  '''
  df = dataframe.copy()
  nan_value = float('NaN')
  df.replace('', nan_value, inplace=True)
  df.replace(' ', nan_value, inplace=True)
  df.replace('', np.nan, inplace=True)
  df.dropna(subset = [content_column], inplace=True)
  df = df[df[content_column] != '']
  return df


def drop_duplicates(dataframe, content_column):
  '''Parameters:
  dataframe: dataframe with data,

  content_column: str - name of the column in the dataframe based on which you want to drop duplicated values,
  '''
  df = dataframe.copy()
  df.drop_duplicates(subset=[content_column], keep='first', inplace=True)
  return df


def lemmatization(dataframe, text_column):
  '''Parameters:
  dataframe: dataframe with your data,

  text_column: column of a dataframe where text is located
  '''
  df = dataframe.copy()
  lemmas = []
  for doc in nlp.pipe(df[text_column].apply(str)):
#    lemmas.append([token.lemma_ for token in doc]) #basic tokenization an dlemmatization without removing anything
    lemmas.append([token.lemma_ for token in doc if not (token.is_punct or token.like_num) and (len(token) > 2)])
  df[text_column +"_lemmatized"] = lemmas
  df[df[text_column +"_lemmatized"].map(len) > 0]
  return df


def find_emotive_words(dataframe, content_lemmatized_column, affective_database_path, db_words = "Word", uniq_words=False):
  '''Parameters:
  dataframe: dataframe with your data,

  content_lemmatized_column: str - name of a column in dataframe where lemmatized text is located,

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed,

  uniq_words: boolean - True if you want to retrieve only unique emotive words from your text data,
  False if you want to retrieve every emotive word (thus, there can be duplicated words),
  --> *by default it is set to False
  '''
  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  affective_database = affective_database[[db_words]]

  all_emotive_words = []
  if uniq_words == True:
    for lemmas_list in dataframe[content_lemmatized_column]:
      emotive_words = [word for word in set(lemmas_list).intersection(affective_database[db_words])]
      all_emotive_words.append(emotive_words)

  elif uniq_words == False:
    for lemmas_list in dataframe[content_lemmatized_column]:
      from collections import Counter
      list_words = list(affective_database[db_words])
      list_text = pd.Series(lemmas_list)
      words_in_database = Counter(list_words)
      lemma_words = Counter(list_text)
      emotive_words = [key for key in list(lemma_words.keys()) if key in list(words_in_database.keys()) for i in range(lemma_words[key])]
      all_emotive_words.append(emotive_words)

  dataframe["Emotive_words"] = all_emotive_words
  return dataframe


# we take merged lexicon (Emean + NAWL --> "joined_scaled_NAWL-Sentimenti_db.xlsx"") with normalized values
def average_joined_lexicons(dataframe, emotive_words_column, affective_database_path, db_words):
  '''Parameters:
  dataframe: dataframe with your data,

  emotive_words_column: str - name of a column in dataframe where emotive words are listed
  (returned from the previous function),

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed
  '''

  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  emotion_values = ['Happiness', 'Anger', 'Sadness', 'Fear', 'Disgust', 'Valence', 'Arousal']
  used_cols = [db_words] + emotion_values

  affective_database = affective_database[used_cols]
  affective_database.set_index(db_words, inplace=True)

  happ_all = []
  ang_all = []
  sad_all = []
  fea_all = []
  dis_all = []
  val_all = []
  aro_all = []

  happ_all_vals = []
  ang_all_vals = []
  sad_all_vals = []
  fea_all_vals = []
  dis_all_vals = []
  val_all_vals = []
  aro_all_vals = []

  for emotive_words in dataframe[emotive_words_column]:
    individual_scores = []
    values_scores = []
    for emotion_value in emotion_values:
      individual = affective_database.loc[emotive_words][emotion_value].to_numpy(dtype=np.float32).flatten()
      individual_scores.append(individual)

      average = round(np.nanmean(np.array(individual)), 5)
      values_scores.append(average)

    happ_ind = individual_scores[0]
    happ_all.append(list(happ_ind))
    ang_ind = individual_scores[1]
    ang_all.append(list(ang_ind))
    sad_ind = individual_scores[2]
    sad_all.append(list(sad_ind))
    fea_ind = individual_scores[3]
    fea_all.append(list(fea_ind))
    dis_ind = individual_scores[4]
    dis_all.append(list(dis_ind))
    val_ind = individual_scores[5]
    val_all.append(list(val_ind))
    aro_ind = individual_scores[6]
    aro_all.append(list(aro_ind))

    happ_val = values_scores[0]
    happ_all_vals.append(happ_val)
    ang_val = values_scores[1]
    ang_all_vals.append(ang_val)
    sad_val = values_scores[2]
    sad_all_vals.append(sad_val)
    fea_val = values_scores[3]
    fea_all_vals.append(fea_val)
    dis_val = values_scores[4]
    dis_all_vals.append(dis_val)
    val_val = values_scores[5]
    val_all_vals.append(val_val)
    aro_val = values_scores[6]
    aro_all_vals.append(aro_val)

  dataframe["Happiness"] = happ_all_vals
  dataframe["Anger"] = ang_all_vals
  dataframe["Sadness"] = sad_all_vals
  dataframe["Fear"] = fea_all_vals
  dataframe["Disgust"] = dis_all_vals
  dataframe["Valence"] = val_all_vals
  dataframe["Arousal"] = aro_all_vals

  dataframe["Happiness_individual_values"] = happ_all
  dataframe["Anger_individual_values"] = ang_all
  dataframe["Sadness_individual_values"] = sad_all
  dataframe["Fear_individual_values"] = fea_all
  dataframe["Disgust_individual_values"] = dis_all
  dataframe["Valence_individual_values"] = val_all
  dataframe["Arousal_individual_values"] =  aro_all
  return dataframe


# function for calculating the difference from the mean
def baseline_diff_avg(emotion_value, emotion, emotion_avg):
  '''Parameters:
  emotion_value: float - the emotion value from which the mean for the patricular emotion will be subtracted from,

  emotion: str - column name where the emotion value is listed,

  emotion_avg: dataframe - computed baseline values
  '''
  corrected_value = round(emotion_value - emotion_avg[emotion].iloc[0], 5)
  return corrected_value


# here we select time range, resample data to 1 minute units
# and compute the baseline which will be used later (in next function)
def resample_and_compute_baseline(input_dataframe, date_col, time_from, time_to, time_unit = "1 min"):
  '''Parameters:
  input_dataframe: your dataframe name with data,

  date_col: str - column name where datetime is,

  time_from: str - starting time of the range you want to analyze
  in format "year-month-day hour:minutes:seconds",
  e.g. '2019-10-08 19:30:00',

  time_to: str - ending time of the range you want to analyze
  in format "year-month-day hour:minutes:seconds",
  e.g. '2019-10-08 19:30:00',

  time_unit: str - time unit for analysis
  *please follow instructions specified below to correctly set this parameter;
  if you want time_unit to be number of  seconds,
  then specify number of seconds and use  sec  abbreviation,
  e.g. for 30 seconds unit it looks like this:   time_unit = "30 sec"

  if you want time_unit to be number of  minutes,
  then specify number of minutes and use  min  abbreviation,
  e.g. for 5 minute unit it looks like this:   time_unit = "5 min"

  if you want time_unit to be number of  hours,
  then specify number of hours and use  h  abbreviation,
  e.g. for 2 hours unit it looks like this:   time_unit = "2 h"

  if you want time_unit to be number of  days,
  then specify number of seconds and use  day  abbreviation,
  e.g. for 7 days unit it looks like this:   time_unit = "7 day",

   --> *by default time_unit is set to 1 minute


  ** function returns:
  firstly - dataframe with average values for each emotion computed for each minute of specified time range,
  secondly - baseline values in a form of dataframe with one row with emotions baselines
  (mean values for each emotion for the whole analyzed dataset)
  '''
  df = input_dataframe.copy()

  if "min" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("min", "T")
  elif "sec" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("sec", "S")
  elif "h" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("h", "H")
  elif "day" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("day", "D")

  emotions_cols = ['Arousal', 'Anger', 'Sadness', 'Disgust', 'Valence', 'Fear', 'Happiness']
  columns_to_analyze = [date_col] + emotions_cols
  df = df[columns_to_analyze]
  df[date_col] = pd.to_datetime(df[date_col], dayfirst = True)
  df = df[(df[date_col] >= time_from) & (df[date_col] <= time_to)].sort_values(by = date_col)

  df = df.rename(columns={date_col:'Time'})
  df_described = df[emotions_cols].describe()
  df_described_mean = pd.DataFrame(df_described.loc["mean"]).T

  melt_data = df.melt('Time', var_name='Emotions', value_name='Values')
  melt_data = melt_data.sort_values("Time")

  data_resampled_aro = melt_data[melt_data.Emotions == "Arousal"].set_index("Time").resample(time_unit)['Values'].mean().fillna(0)
  data_resampled_ang = melt_data[melt_data.Emotions == "Anger"].set_index("Time").resample(time_unit)['Values'].mean().fillna(0)
  data_resampled_sad = melt_data[melt_data.Emotions == "Sadness"].set_index("Time").resample(time_unit)['Values'].mean().fillna(0)
  data_resampled_dis = melt_data[melt_data.Emotions == "Disgust"].set_index("Time").resample(time_unit)['Values'].mean().fillna(0)
  data_resampled_val = melt_data[melt_data.Emotions == "Valence"].set_index("Time").resample(time_unit)['Values'].mean().fillna(0)
  data_resampled_fea = melt_data[melt_data.Emotions == "Fear"].set_index("Time").resample(time_unit)['Values'].mean().fillna(0)
  data_resampled_hap = melt_data[melt_data.Emotions == "Happiness"].set_index("Time").resample(time_unit)['Values'].mean().fillna(0)
  data_frames_to_concat = [data_resampled_aro, data_resampled_ang, data_resampled_sad, data_resampled_dis, data_resampled_val, data_resampled_fea, data_resampled_hap]

  data_resampled_all = pd.concat(data_frames_to_concat, join='outer', axis=1).fillna(0)
  data_resampled_all.columns = ['Arousal', 'Anger', 'Sadness', 'Disgust', 'Valence', 'Fear', 'Happiness']
  data_resampled_all = data_resampled_all.reset_index()
  data_resampled_all["Date"] = data_resampled_all["Time"].dt.date
  data_resampled_all["Time"] = data_resampled_all["Time"].dt.time.apply(str)

  return data_resampled_all, df_described_mean



# returns the difference value for each emotion from baseline value which is the mean value for the whole dataset in selected time range for a particular emotion)
## that is, the new value shows how much it differs from the overall mean value for the whole dataset
def difference_from_baseline(input_dataframe, baseline):
  '''Parameters:
  input_dataframe - dataframe returned from the previous function ("resample_and_compute_baseline")
  where emotions (mean) values are grouped by time,

  baseline: dataframe with baseline values for emotions returned from the previous function ("resample_and_compute_baseline"),


  ** function returns:
  dataframe with values computed as a measure of difference from the baseline value
  '''
  data_resampled_all = input_dataframe.copy()

  data_resampled_all["diff_from_baseline_Arousal"] = data_resampled_all["Arousal"].apply(baseline_diff_avg, emotion = "Arousal", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Anger"] = data_resampled_all["Anger"].apply(baseline_diff_avg, emotion = "Anger", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Sadness"] = data_resampled_all["Sadness"].apply(baseline_diff_avg, emotion = "Sadness", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Disgust"] = data_resampled_all["Disgust"].apply(baseline_diff_avg, emotion = "Disgust", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Valence"] = data_resampled_all["Valence"].apply(baseline_diff_avg, emotion = "Valence", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Fear"] = data_resampled_all["Fear"].apply(baseline_diff_avg, emotion = "Fear", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Happiness"] = data_resampled_all["Happiness"].apply(baseline_diff_avg, emotion = "Happiness", emotion_avg = baseline)
  return data_resampled_all


# returns two difference values for each emotion from 2 different baselines:
#   1) first value is a difference from the mean value for the whole dataset in selected time range for a particular emotion)
## that is, the first new value shows how much it differs from the overall mean value for the whole dataset
#   2) second value is a difference from the previous value (value from the previous minute) for a particular emotion)
def two_difference_from_baseline(input_dataframe, baseline):
  '''Parameters:
  input_dataframe - dataframe returned from the previous function ("resample_and_compute_baseline")
  where emotions (mean) values are grouped by time,

  baseline: dataframe with baseline values for emotions returned from the previous function ("resample_and_compute_baseline"),


  ** function returns:
  dataframe with 2 new values for each emotion:
  1) "diff_from_baseline_": value computed as a measure of difference from the baseline value,
  2) "diff_from_previous_value_": value computed as a measure of difference from the previous value (i.e., value from the previous minute)
  '''
  data_resampled_all = input_dataframe.copy()

  data_resampled_all["diff_from_baseline_Arousal"] = data_resampled_all["Arousal"].apply(baseline_diff_avg, emotion = "Arousal", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Anger"] = data_resampled_all["Anger"].apply(baseline_diff_avg, emotion = "Anger", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Sadness"] = data_resampled_all["Sadness"].apply(baseline_diff_avg, emotion = "Sadness", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Disgust"] = data_resampled_all["Disgust"].apply(baseline_diff_avg, emotion = "Disgust", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Valence"] = data_resampled_all["Valence"].apply(baseline_diff_avg, emotion = "Valence", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Fear"] = data_resampled_all["Fear"].apply(baseline_diff_avg, emotion = "Fear", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Happiness"] = data_resampled_all["Happiness"].apply(baseline_diff_avg, emotion = "Happiness", emotion_avg = baseline)

  difference_from_prev_value = data_resampled_all[['Arousal', 'Anger', 'Sadness', 'Disgust', 'Valence', 'Fear','Happiness']].diff().fillna(0).round(5)
  difference_from_prev_value.columns = ['diff_from_previous_value_Arousal', 'diff_from_previous_value_Anger',
                                        'diff_from_previous_value_Sadness', 'diff_from_previous_value_Disgust',
                                        'diff_from_previous_value_Valence', 'diff_from_previous_value_Fear',
                                        'diff_from_previous_value_Happiness']
  data_resampled_all = pd.merge(data_resampled_all, difference_from_prev_value, left_index=True, right_index=True, how="left")
  return data_resampled_all


def emotion_category(dataframe, emotive_words_column, affective_database_path, db_words = "Word", db_emotion_category = "Class"):
  '''Parameters:
  dataframe: dataframe with your data,

  content_lemmatized_column: str - name of a column in dataframe where lemmatized text is located,

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed,

  db_emotion_category: str - name of the column from affective database from where the categories will be taken
  '''

  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  affective_database = affective_database[[db_words, db_emotion_category]]
  affective_database.set_index(db_words, inplace=True)
  set_of_words = set(affective_database.index)

  all_emotion_categories = []
  for emotive_words in dataframe[emotive_words_column]:
    emotion_categories = [affective_database[db_emotion_category].loc[str(word)] if str(word) in set_of_words else np.nan for word in emotive_words]
    all_emotion_categories.append(emotion_categories)
  dataframe["Emotion_categories"] = all_emotion_categories
  return dataframe


def count_categories(dataframe, emotion_categories_column, affective_database_path, db_emotion_category):
  '''Parameters:
  dataframe: dataframe with data,

  emotion_categories_column: str - name of a column in dataframe where the list of emotion categories assigned to emotive words is located,

  affective_database_path: str - path to a file with affective database,

  db_emotion_category: str - name of the column from affective database from where the categories will be taken
  '''
  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  all_categories = affective_database[db_emotion_category].unique().tolist()

  dataframe["merge_indx"] = range(0, len(dataframe))
  from collections import Counter
  dataframe = pd.merge(dataframe, pd.DataFrame([Counter(x) for x in dataframe[emotion_categories_column]]).fillna(0).astype(int).add_prefix("CATEGORY_"), how='left', left_on="merge_indx", right_index=True)
  dataframe.drop(["merge_indx"], axis=1, inplace=True)

  for category in all_categories:
    if not "CATEGORY_"+category in dataframe.columns:
      dataframe["CATEGORY_"+category] = 0
  return dataframe


def plot_emotion_changes(dataframe, time_column = "Date", plot_title = "Emotions changes", ticks_unit = 7, axis_grid = "x"):
  '''
  Plot all 4 categories in 1 figure - anger, happiness, fear and valence
  '''
  sns.set_theme(style="whitegrid")
  plt.style.use("seaborn-talk")
  fig, ax1 = plt.subplots(1, 1, figsize=(20, 8.5))
  ax1.plot(dataframe[time_column], dataframe["diff_from_baseline_Fear"], c="#000000", label = "Fear")
  ax1.plot(dataframe[time_column], dataframe["diff_from_baseline_Valence"], c="#D81313", label = "Valence")
  ax1.plot(dataframe[time_column], dataframe["diff_from_baseline_Anger"], c="#FD7E00", label = "Anger")
  ax1.plot(dataframe[time_column], dataframe["diff_from_baseline_Happiness"], c='#8DF903', label = "Happiness", alpha=0.8)
  ax1.plot(dataframe[time_column], dataframe["diff_from_baseline_Sadness"], c='#010598', label = "Sadness", alpha=0.8)
  ax1.plot(dataframe[time_column], dataframe["diff_from_baseline_Arousal"], c='#01840E', label = "Arousal")
  ax1.plot(dataframe[time_column], dataframe["diff_from_baseline_Disgust"], c='#B800B2', label = "Disgust", alpha=0.8)

  x = list(dataframe[time_column])
  ax1.set_xticks(x[::ticks_unit])
  ax1.set_xticklabels(x[::ticks_unit], rotation=90)
  ax1.set_title(plot_title+"\n\n", fontsize=17)
  ax1.set_xlabel("\n"+"Date", fontsize=15)
  ax1.set_ylabel("Value", fontsize=15)
  ax1.grid(axis=axis_grid)
  #plt.tight_layout()
  # new legend position
  plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.075), ncol=7)
  #plt.legend()
  plt.show()


#     ********  TO DO - ADJUST Y-axis VARIABLES  --> transform it to .melt() version in a function **********
#  ticks unit showed !!!
"""
melt_data_selected_time = adjusted_data_selected_time[['Date', 'diff_from_baseline_Arousal', 'diff_from_baseline_Anger', 'diff_from_baseline_Sadness',
       'diff_from_baseline_Disgust', 'diff_from_baseline_Valence', 'diff_from_baseline_Fear', 'diff_from_baseline_Happiness']]
melt_data_selected_time.columns = ['Date', 'Arousal', 'Anger', 'Sadness', 'Disgust', 'Valence', 'Fear', 'Happiness']
melt_data_selected_time.head(2)

melt_data_selected_time = melt_data_selected_time.melt('Date', var_name="Emotions", value_name='Value')
melt_data_selected_time.head(3)


melt_data_selected_time["Date"] = melt_data_selected_time["Date"].apply(str)
melt_data_selected_time.head(2)
"""
def plot_individual_emotion(dataframe, time_column = "Date", plot_title = "Emotions changes coputed with the use of baseline"):
  '''
  Plot values for each emotion separately
  '''
  sns.set_theme(style="whitegrid")
  plt.style.use("seaborn-talk")

  dict_colors = {'Happiness' : '#8DF903', 'Anger' : '#FD7E00', 'Sadness' : '#010598', 'Fear' : '#000000', 'Disgust' : '#B800B2', 'Valence' : '#D81313', 'Arousal' : '#01840E'}

  x2 = sns.FacetGrid(dataframe, col="Emotions", palette = dict_colors, hue='Emotions', height=8.15, aspect=2.25, sharex=False, col_wrap=4)
  x2.map(sns.lineplot, "Date", "Value")

  for ax in x2.axes.flat:
      labels = ax.get_xticklabels() # get x labels
      for i,l in enumerate(labels):
          if(i%2 == 0): labels[i] = ' ' # skip  labels
          elif(i%3 == 0): labels[i] = ' ' # skip  labels
          elif(i%3 == 1): labels[i] = ' ' # skip  labels
          #elif(i%11 == 0): labels[i] = ' ' # skip  labels  -->   check these labels and adjust them
      ax.set_xticklabels(labels, rotation=90, fontsize=10) # set new labels
      #ax.grid(False)
      ax.grid(axis='x')
  plt.tight_layout()
  plt.show()


def get_polarity_score(dataframe, content_lemmatized_column, affective_database_path, db_words = "Word"):
  '''Parameters:
  dataframe: dataframe with your data,

  content_lemmatized_column: str - name of a column in dataframe where words-lemmas are listed

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed
  '''
  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  emotion_values = ["Polarity"]
  used_cols = [db_words] + emotion_values

  affective_database_polarity = affective_database[used_cols]
  affective_database_polarity.set_index(db_words, inplace=True)

  all_polarity_scores = []
  all_neg_scores_counts = []
  all_pos_scores_counts = []
  all_neg_scores = []
  all_pos_scores = []
  all_neg_percent = []
  all_pos_percent = []

  for lemmas_list in dataframe[content_lemmatized_column]:
    emotive_words = [word for word in set(lemmas_list).intersection(affective_database[db_words])]

    length_emo_words_list = len(emotive_words)
    if length_emo_words_list > 0:
      scores = affective_database_polarity.loc[emotive_words]

      polarity_score = (scores.sum()[0])
      all_polarity_scores.append(polarity_score)

      neg_scores_count = scores.where(scores["Polarity"] < 0).count()[0]
      all_neg_scores_counts.append(neg_scores_count)

      pos_scores_count = scores.where(scores["Polarity"] > 0).count()[0]
      all_pos_scores_counts.append(pos_scores_count)

      neg_score = abs(np.sum(scores.where(scores["Polarity"] < 0))[0])
      all_neg_scores.append(neg_score)

      pos_score = np.sum(scores.where(scores["Polarity"] > 0))[0]
      all_pos_scores.append(pos_score)

      neg_percent = round((neg_scores_count / length_emo_words_list), 3)
      all_neg_percent.append(neg_percent)

      pos_percent = round((pos_scores_count / length_emo_words_list), 3)
      all_pos_percent.append(pos_percent)

    else:
      polarity_score=neg_scores_count=pos_scores_count=neg_score=pos_score=neg_percent=pos_percent = np.NaN
      all_polarity_scores.append(polarity_score)
      all_neg_scores_counts.append(neg_scores_count)
      all_pos_scores_counts.append(pos_scores_count)
      all_neg_scores.append(neg_score)
      all_pos_scores.append(pos_score)
      all_neg_percent.append(neg_percent)
      all_pos_percent.append(pos_percent)

  dataframe["Polarity_score"] = all_polarity_scores
  dataframe["Negative_polarity_count"] = all_neg_scores_counts
  dataframe["Positive_polarity_count"] = all_pos_scores_counts
  dataframe["Negative_polarity_score"] = all_neg_scores #maybe change to absolute value so easier to plot later in one figure (i.e., on similar scale)
  dataframe["Positive_polarity_score"] = all_pos_scores
  dataframe["Negative_percentage"] = all_neg_percent
  dataframe["Positive_percentage"] = all_pos_percent
  return dataframe



def universal_resample_polarity(input_dataframe, date_col, polarity_col, time_from, time_to, time_unit = "1 min"):
  '''Parameters:
  input_dataframe: your dataframe name with data,

  date_col: str - column name where datetime is,

  polarity_col: str - column name with polarity score you want to use,

  time_from: str - starting time of the range you want to analyze
  in format "year-month-day hour:minutes:seconds",
  e.g. '2019-10-08 19:30:00',

  time_to: str - ending time of the range you want to analyze
  in format "year-month-day hour:minutes:seconds",
  e.g. '2019-10-08 19:30:00',

  time_unit: str - time unit for analysis
  *please follow instructions specified below to correctly set this parameter;
  if you want time_unit to be number of  seconds,
  then specify number of seconds and use  sec  abbreviation,
  e.g. for 30 seconds unit it looks like this:   time_unit = "30 sec"

  if you want time_unit to be number of  minutes,
  then specify number of minutes and use  min  abbreviation,
  e.g. for 5 minute unit it looks like this:   time_unit = "5 min"

  if you want time_unit to be number of  hours,
  then specify number of hours and use  h  abbreviation,
  e.g. for 2 hours unit it looks like this:   time_unit = "2 h"

  if you want time_unit to be number of  days,
  then specify number of seconds and use  day  abbreviation,
  e.g. for 7 days unit it looks like this:   time_unit = "7 day",

   --> *by default time_unit is set to 1 minute


  ** function returns:
  firstly - dataframe with average values for each emotion computed for each minute of specified time range,
  secondly - baseline values in a form of dataframe with one row with emotions baselines
  (mean values for each emotion for the whole analyzed dataset)
  '''
  df = input_dataframe.copy()

  if "min" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("min", "T")
  elif "sec" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("sec", "S")
  elif "h" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("h", "H")
  elif "day" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("day", "D")

  emotions_cols = [polarity_col]
  columns_to_analyze = [date_col] + emotions_cols
  df = df[columns_to_analyze]
  df[date_col] = pd.to_datetime(df[date_col], dayfirst = True)
  df = df[(df[date_col] >= time_from) & (df[date_col] <= time_to)].sort_values(by = date_col)

  df = df.rename(columns={date_col:'Time'})
  data_resampled = df.set_index("Time").resample(time_unit).mean().fillna(0)
  data_resampled = data_resampled.reset_index()
  data_resampled["Date"] = data_resampled["Time"].dt.date
  data_resampled["Time"] = data_resampled["Time"].dt.time.apply(str)
  data_resampled.columns = ["Time", polarity_col+"_mean", "Date"]
  return data_resampled




df = load_data("/content/drive/MyDrive/Colab Notebooks/twitter/NAWL_EMEAN_2nd_round/tweets_Debata_CzasDecyzji_Emean_AVG_CAT.xlsx")

df_selected_time, baseline_value = resample_and_compute_baseline(input_dataframe = df,
                                                                 date_col = "Data opublikownia wzmianki",
                                                                 time_from = '2019-10-08 19:30:00',
                                                                 time_to = '2019-10-08 23:00:00', time_unit = "1 min")

df_selected_time = two_difference_from_baseline(df_selected_time, baseline = baseline_value)
print(df_selected_time.tail(3))


sns.set_theme(style="whitegrid")
plt.style.use("seaborn-talk")
fig, ax1 = plt.subplots(1, 1, figsize=(17, 7.5))
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_baseline_Fear"], c="#000000", label = "Fear")
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_baseline_Valence"], c="#D81313", label = "Valence")
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_baseline_Anger"], c="#FD7E00", label = "Anger")
#ax1.plot(df_selected_time.Time, df_selected_time["diff_from_baseline_Arousal"], c='#01840E', label = "Arousal")
#ax1.plot(df_selected_time.Time, df_selected_time["diff_from_baseline_Sadness"], c='#010598', label = "Sadness", alpha=0.8)
#ax1.plot(df_selected_time.Time, df_selected_time["diff_from_baseline_Disgust"], c='#B800B2', label = "Disgust", alpha=0.8)
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_baseline_Happiness"], c='#8DF903', label = "Happiness", alpha=0.8)

x = list(df_selected_time.Time)
ax1.set_xticks(x[::5])
ax1.set_xticklabels(x[::5], rotation=90, fontsize=12)
ax1.set_title("Emotions changes in TVP May 2020 Twitter"+"\n\n", fontsize=20)
ax1.set_xlabel("\nTime", fontsize=16)
ax1.set_ylabel("Value", fontsize=16)
ax1.grid(axis='x')
plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.085), ncol=7)
plt.show()



sns.set_theme(style="whitegrid")
plt.style.use("seaborn-talk")
fig, ax1 = plt.subplots(1, 1, figsize=(17, 7.5))
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_previous_value_Fear"], c="#000000", label = "Fear")
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_previous_value_Valence"], c="#D81313", label = "Valence")
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_previous_value_Anger"], c="#FD7E00", label = "Anger")
ax1.plot(df_selected_time.Time, df_selected_time["diff_from_previous_value_Happiness"], c='#8DF903', label = "Happiness", alpha=0.68)

x = list(df_selected_time.Time)
ax1.set_xticks(x[::5])
ax1.set_xticklabels(x[::5], rotation=90, fontsize=12)
ax1.set_title("Emotions changes in TVP May 2020 Twitter"+"\n\n", fontsize=20)
ax1.set_xlabel("\nTime", fontsize=16)
ax1.set_ylabel("Value", fontsize=16)
ax1.grid(axis='x')
plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.085), ncol=7)
plt.show()




df_test_polar1 = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="Negative_polarity_score",
                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "1 min")
df_test_polar1.head(1)

df_test_polar = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="Positive_polarity_score",
                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "1 min")
df_test_polar.head(1)

fig, ax1 = plt.subplots(1, 1, figsize=(12, 7))
x = list(df_test_polar.Time)
ax1.plot(df_test_polar["Time"], df_test_polar["Positive_polarity_score_mean"], label = "Positive polarity score", color = "#15B3AF")
ax1.plot(df_test_polar1["Time"], df_test_polar1["Negative_polarity_score_mean"], label = "Negative polarity score", color = "#840004")
ax1.set_xticks(x[::5])
ax1.set_xticklabels(x[::5], rotation=90)
ax1.set_xlabel("\nTime")
ax1.set_title("Polarity scores in TVP 19 tweets\n\n", fontsize = 15)
ax1.set_ylabel("Score\n")
ax1.grid(axis="x")
plt.ylim(0, 4)
plt.tight_layout()
plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.1), ncol=2)
plt.show()


dis_cat_df = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="CATEGORY_DIS",
                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "1 min")

fea_cat_df = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="CATEGORY_FEA",
                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "1 min")

SAD_cat_df = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="CATEGORY_SAD",
                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "1 min")

HAP_cat_df = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="CATEGORY_HAP",
                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "1 min")

ANG_cat_df = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="CATEGORY_ANG",
                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "1 min")


fig, ax2 = plt.subplots(1, 1, figsize=(14.5, 8))
x2 = list(fea_cat_df.Time)
ax2.plot(fea_cat_df["Time"], fea_cat_df["CATEGORY_FEA_mean"], label = "Fear", color = "#000000", alpha = 0.75)
ax2.plot(dis_cat_df["Time"], dis_cat_df["CATEGORY_DIS_mean"], label = "Disgust", color = "#B800B2", alpha = 0.75)
ax2.plot(SAD_cat_df["Time"], SAD_cat_df["CATEGORY_SAD_mean"], label = "Sadness", color = "#010598", alpha = 0.75)
ax2.plot(HAP_cat_df["Time"], HAP_cat_df["CATEGORY_HAP_mean"], label = "Happiness", color = "#8DF903", alpha = 0.75)
ax2.plot(ANG_cat_df["Time"], ANG_cat_df["CATEGORY_ANG_mean"], label = "Anger", color = "#FD7E00", alpha = 0.75)
ax2.set_xticks(x2[::5])
ax2.set_xticklabels(x2[::5], rotation=90)
ax2.set_xlabel("\nTime")
ax2.set_ylabel("Count\n")
ax2.set_title("Number of words assigned to each emotion category (mean from minute) in TVP 19 tweets\n\n", fontsize = 15)
ax2.grid(axis="x")
plt.tight_layout()
plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.1), ncol=5)
plt.show()



#calling functions


tweets_czas = load_data("/content/drive/MyDrive/Colab Notebooks/twitter/Debata_czerwiec - Lista wyników od 16.06.2020 do 17.06.2020.xlsx", indx=False)

cln_tweets_czas = clean_text(tweets_czas, 'Tekst wzmianki')

with timer():
  cln_tweets_czas = drop_empty_content(cln_tweets_czas, content_column = 'clean_Tekst wzmianki')

cln_tweets_czas = drop_duplicates(cln_tweets_czas, content_column = 'clean_Tekst wzmianki')

with timer():
  cln_tweets_czas = lemmatization(cln_tweets_czas, "clean_Tekst wzmianki")

with timer():
  cln_tweets_czas = find_emotive_words(cln_tweets_czas, content_lemmatized_column = "clean_Tekst wzmianki_lemmatized",
                                           affective_database_path = "/content/drive/MyDrive/Colab Notebooks/Emotional word lists/joined_scaled_NAWL-Sentimenti_db.xlsx",
                                           db_words = "Word")

with timer():
  cln_tweets_czas = average_joined_lexicons(cln_tweets_czas, emotive_words_column = "Emotive_words",
                                                affective_database_path = "/content/drive/MyDrive/Colab Notebooks/Emotional word lists/joined_scaled_NAWL-Sentimenti_db.xlsx",
                                                db_words = "Word")


# dedicated database for categories
# /content/drive/MyDrive/Colab Notebooks/Emotional word lists/emotion_6-categories_NAWL_Sentimenti_db.xlsx
with timer():
  cln_tweets_czas = emotion_category(dataframe = cln_tweets_czas, emotive_words_column = "Emotive_words",
                                 affective_database_path = "/content/drive/MyDrive/Colab Notebooks/Emotional word lists/emotion_6-categories_NAWL_Sentimenti_db.xlsx")

print(cln_tweets_czas.head(2))



data_selected_time, baseline_value = resample_and_compute_baseline(input_dataframe = tweets_czas, date_col = 'Data opublikowania wzmianki',
                                                                     time_from = '2019-10-01 20:00:00', time_to = '2019-10-01 22:00:00', time_unit = "30 sec")

# we can check how baseline values look
print(baseline_value)

data_selected_time = two_difference_from_baseline(data_selected_time, baseline = baseline_value)
print(data_selected_time.head(3))
