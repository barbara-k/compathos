# -*- coding: utf-8 -*-
"""more_efficient_analysis.ipynb
Automatically generated by Colaboratory.

analysis pipe:
1 - text cleaning - basics: punctuation marks, , urls, \n (new lines), hashtags, users' mentions
2 - spacy language model
3 - find emotive words
4 - assign categories to those found emotive words
5 - take avg values scores for emotions and individual values for each emotive word
6 - time series analysis --> choosing time range, resample to time units, avg scores for time units
7 - computing baseline values and differences from baselines
8 - ploting results
"""

import pandas as pd
pd.set_option("max_colwidth", 200)
import datetime
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")
plt.style.use("seaborn-talk")

import warnings
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
warnings.simplefilter(action='ignore', category=DeprecationWarning)
np.seterr(divide='ignore')
warnings.filterwarnings(action='ignore', message='Mean of empty slice')
pd.options.mode.chained_assignment = None  # default='warn'


import spacy
# load SPACY model
nlp = spacy.load('pl_core_news_sm')

from timeit import default_timer
from contextlib import contextmanager

@contextmanager
def timer():
    start_time = default_timer()
    try:
        yield
    finally:
        print("Time taken to execute the function:\n--->  %s seconds  <---\n" % (default_timer() - start_time))


def load_data(file_path, indx = True, indx_col = 0):
  '''Parameters:
  file_path: path to your excel or csv file with data,
  indx: boolean - whether there is index column in your file (usually it is the first column) --> *by default it is set to True
  indx_col: int - if your file has an index column, specify column number here --> *by default it is equal to 0 (first column)
  '''
  if indx == True and file_path.endswith(".xlsx"):
    data = pd.read_excel(file_path, index_col = indx_col)
  elif indx == False and file_path.endswith(".xlsx"):
    data = pd.read_excel(file_path)

  elif indx == True and file_path.endswith(".csv"):
    data = pd.read_csv(file_path, index_col = indx_col)
  elif indx == False and file_path.endswith(".csv"):
    data = pd.read_csv(file_path)
  return data

# clean tweeets text from urls, mentions, punctuation marks
def clean_text(dataframe, text_column):
  '''Parameters:
  dataframe: dataframe with data,
  text_column: str - name of the column in the dataframe where the text you want to clean is listed
  '''
  import re
  import string
  df = dataframe.copy()
  all_texts = []
  for text in df[text_column]:
    text = re.sub(r"(http|https):\/\/([\w\s\d\.]+)(\/?)(.*)", "", str(text)) # urls
    text = re.sub('@[\w]+',' ', text)  # mentions
    text = text.replace("\n", " ") # new lines
    text = re.sub(r'\B#\w*[a-zA-Z]+\w*',' ', text) # hashtags
    text = text.translate(str.maketrans(' ', ' ', string.punctuation)) # punctuation marks
    text = text.strip()
    text = re.sub(r'\s\s\s+', ' ', text) # empty spaces
    all_texts.append(text)
  df["clean_" + text_column] = all_texts
  return df


# function to drop NaN (empty) rows from your data
def drop_empty_content(dataframe, content_column):
  '''Parameters:
  dataframe: dataframe with data,

  content_column: str - name of the column in the dataframe based on which you want to drop missing values,
  --> (*hint: it should be cleaned_text column returned from the previous function or just a column containing text data)
  '''
  df = dataframe.copy()
  nan_value = float('NaN')
  df.replace('', nan_value, inplace=True)
  df.replace(' ', nan_value, inplace=True)
  df.replace('', np.nan, inplace=True)
  df.dropna(subset = [content_column], inplace=True)
  df = df[df[content_column] != '']
  return df


def drop_duplicates(dataframe, content_column):
  '''Parameters:
  dataframe: dataframe with data,

  content_column: str - name of the column in the dataframe based on which you want to drop duplicated values,
  '''
  df = dataframe.copy()
  df.drop_duplicates(subset=[content_column], keep='first', inplace=True)
  return df


def lemmatization(dataframe, text_column):
  '''Parameters:
  dataframe: dataframe with your data,

  text_column: column of a dataframe where text is located
  '''
  df = dataframe.copy()
  lemmas = []
  for doc in nlp.pipe(df[text_column].apply(str)):
    lemmas.append([token.lemma_ for token in doc if not (token.is_punct or token.like_num) and (len(token) > 2)])
  df[text_column +"_lemmatized"] = lemmas
  df[df[text_column +"_lemmatized"].map(len) > 0]
  return df


def find_emotive_words(dataframe, content_lemmatized_column, affective_database_path, db_words = "Word", uniq_words=False):
  '''Parameters:
  dataframe: dataframe with your data,

  content_lemmatized_column: str - name of a column in dataframe where lemmatized text is located,

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed,

  uniq_words: boolean - True if you want to retrieve only unique emotive words from your text data,
  False if you want to retrieve every emotive word (thus, there can be duplicated words),
  --> *by default it is set to False
  '''
  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  affective_database = affective_database[[db_words]]
  affective_database_emotive_words = affective_database[db_words].tolist()

  all_emotive_words = []
  if uniq_words == True:
    for lemmas_list in dataframe[content_lemmatized_column]:
      emotive_words = [word for word in set(lemmas_list).intersection(affective_database[db_words])]
      all_emotive_words.append(emotive_words)

  elif uniq_words == False:
    for lemmas_list in dataframe[content_lemmatized_column]:
      emotive_words = []
      for word in lemmas_list:
        if word in affective_database_emotive_words:
          emotive_words.append(word)

      all_emotive_words.append(emotive_words)
  
  dataframe["Emotive_words"] = all_emotive_words
  return dataframe


# we take merged lexicon (Emean + NAWL --> "joined_scaled_NAWL-Sentimenti_db.xlsx"") with normalized values
def average_joined_lexicons(dataframe, emotive_words_column, affective_database_path, db_words):
  '''Parameters:
  dataframe: dataframe with your data,

  emotive_words_column: str - name of a column in dataframe where emotive words are listed
  (returned from the previous function),

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed
  '''

  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  emotion_values = ['Happiness', 'Anger', 'Sadness', 'Fear', 'Disgust', 'Valence', 'Arousal']
  used_cols = [db_words] + emotion_values

  affective_database = affective_database[used_cols]
  affective_database.set_index(db_words, inplace=True)

  happ_all = []
  ang_all = []
  sad_all = []
  fea_all = []
  dis_all = []
  val_all = []
  aro_all = []

  happ_all_vals = []
  ang_all_vals = []
  sad_all_vals = []
  fea_all_vals = []
  dis_all_vals = []
  val_all_vals = []
  aro_all_vals = []

  for emotive_words in dataframe[emotive_words_column]:
    individual_scores = []
    values_scores = []
    for emotion_value in emotion_values:
      individual = affective_database.loc[emotive_words][emotion_value].to_numpy(dtype=np.float32).flatten()
      individual_scores.append(individual)

      average = round(np.nanmean(np.array(individual)), 5)
      values_scores.append(average)

    happ_ind = individual_scores[0]
    happ_all.append(list(happ_ind))
    ang_ind = individual_scores[1]
    ang_all.append(list(ang_ind))
    sad_ind = individual_scores[2]
    sad_all.append(list(sad_ind))
    fea_ind = individual_scores[3]
    fea_all.append(list(fea_ind))
    dis_ind = individual_scores[4]
    dis_all.append(list(dis_ind))
    val_ind = individual_scores[5]
    val_all.append(list(val_ind))
    aro_ind = individual_scores[6]
    aro_all.append(list(aro_ind))

    happ_val = values_scores[0]
    happ_all_vals.append(happ_val)
    ang_val = values_scores[1]
    ang_all_vals.append(ang_val)
    sad_val = values_scores[2]
    sad_all_vals.append(sad_val)
    fea_val = values_scores[3]
    fea_all_vals.append(fea_val)
    dis_val = values_scores[4]
    dis_all_vals.append(dis_val)
    val_val = values_scores[5]
    val_all_vals.append(val_val)
    aro_val = values_scores[6]
    aro_all_vals.append(aro_val)

  dataframe["Happiness"] = happ_all_vals
  dataframe["Anger"] = ang_all_vals
  dataframe["Sadness"] = sad_all_vals
  dataframe["Fear"] = fea_all_vals
  dataframe["Disgust"] = dis_all_vals
  dataframe["Valence"] = val_all_vals
  dataframe["Arousal"] = aro_all_vals

  dataframe["Happiness_individual_values"] = happ_all
  dataframe["Anger_individual_values"] = ang_all
  dataframe["Sadness_individual_values"] = sad_all
  dataframe["Fear_individual_values"] = fea_all
  dataframe["Disgust_individual_values"] = dis_all
  dataframe["Valence_individual_values"] = val_all
  dataframe["Arousal_individual_values"] =  aro_all
  return dataframe


# function for calculating the difference from the mean
def baseline_diff_avg(emotion_value, emotion, emotion_avg):
  '''Parameters:
  emotion_value: float - the emotion value from which the mean for the patricular emotion will be subtracted from,

  emotion: str - column name where the emotion value is listed,

  emotion_avg: dataframe - computed baseline values
  '''
  corrected_value = round(emotion_value - emotion_avg[emotion].iloc[0], 5)
  return corrected_value


# here we select time range, resample data to 1 minute units
# and compute the baseline which will be used later (in next function)
def resample_and_compute_baseline(input_dataframe, date_col, time_from, time_to, time_unit = "1 min"):
  '''Parameters:
  input_dataframe: your dataframe name with data,

  date_col: str - column name where datetime is,

  time_from: str - starting time of the range you want to analyze
  in format "year-month-day hour:minutes:seconds",
  e.g. '2019-10-08 19:30:00',

  time_to: str - ending time of the range you want to analyze
  in format "year-month-day hour:minutes:seconds",
  e.g. '2019-10-08 19:30:00',

  time_unit: str - time unit for analysis
  *please follow instructions specified below to correctly set this parameter;
  if you want time_unit to be number of  seconds,
  then specify number of seconds and use  sec  abbreviation,
  e.g. for 30 seconds unit it looks like this:   time_unit = "30 sec"

  if you want time_unit to be number of  minutes,
  then specify number of minutes and use  min  abbreviation,
  e.g. for 5 minute unit it looks like this:   time_unit = "5 min"

  if you want time_unit to be number of  hours,
  then specify number of hours and use  h  abbreviation,
  e.g. for 2 hours unit it looks like this:   time_unit = "2 h"

  if you want time_unit to be number of  days,
  then specify number of seconds and use  day  abbreviation,
  e.g. for 7 days unit it looks like this:   time_unit = "7 day",

   --> *by default time_unit is set to 1 minute


  ** function returns:
  firstly - dataframe with average values for each emotion computed per time unit of specified time range,
  secondly - baseline values in a form of dataframe with one row with emotions baselines (mean values for each emotion for the whole analyzed dataset)
  
  '''
  df = input_dataframe.copy()

  if "min" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("min", "T")
  elif "sec" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("sec", "S")
  elif "h" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("h", "H")
  elif "day" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("day", "D")

  emotions_cols = ['Arousal', 'Anger', 'Sadness', 'Disgust', 'Valence', 'Fear', 'Happiness']
  columns_to_analyze = [date_col] + emotions_cols
  df = df[columns_to_analyze]
  df[date_col] = pd.to_datetime(df[date_col], dayfirst = True)
  df = df[(df[date_col] >= time_from) & (df[date_col] <= time_to)].sort_values(by = date_col)
  
  df = df.rename(columns={date_col:'Time'})
  df_described = df[emotions_cols].describe()
  df_described_mean = pd.DataFrame(df_described.loc["mean"]).T

  data_resampled_all = df.set_index("Time")[['Arousal', 'Anger', 'Sadness', 'Disgust', 'Valence', 'Fear', 'Happiness']].resample(time_unit).mean().fillna(0)
  data_resampled_all.reset_index(inplace=True)
  data_resampled_all["Date"] = data_resampled_all["Time"].dt.date
  data_resampled_all["Time"] = data_resampled_all["Time"].dt.time.apply(str)

  return data_resampled_all, df_described_mean



# returns the difference value for each emotion from baseline value which is the mean value for the whole dataset in selected time range for a particular emotion)
## that is, the new value shows how much it differs from the overall mean value for the whole dataset
def difference_from_baseline(input_dataframe, baseline):
  '''Parameters:
  input_dataframe - dataframe returned from the previous function ("resample_and_compute_baseline")
  where emotions (mean) values are grouped by time,

  baseline: dataframe with baseline values for emotions returned from the previous function ("resample_and_compute_baseline"),


  ** function returns:
  dataframe with values computed as a measure of difference from the baseline value
  '''
  data_resampled_all = input_dataframe.copy()

  data_resampled_all["diff_from_baseline_Arousal"] = data_resampled_all["Arousal"].apply(baseline_diff_avg, emotion = "Arousal", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Anger"] = data_resampled_all["Anger"].apply(baseline_diff_avg, emotion = "Anger", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Sadness"] = data_resampled_all["Sadness"].apply(baseline_diff_avg, emotion = "Sadness", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Disgust"] = data_resampled_all["Disgust"].apply(baseline_diff_avg, emotion = "Disgust", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Valence"] = data_resampled_all["Valence"].apply(baseline_diff_avg, emotion = "Valence", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Fear"] = data_resampled_all["Fear"].apply(baseline_diff_avg, emotion = "Fear", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Happiness"] = data_resampled_all["Happiness"].apply(baseline_diff_avg, emotion = "Happiness", emotion_avg = baseline)
  return data_resampled_all


# returns two difference values for each emotion from 2 different baselines:
#   1) first value is a difference from the mean value for the whole dataset in selected time range for a particular emotion)
## that is, the first new value shows how much it differs from the overall mean value for the whole dataset
#   2) second value is a difference from the previous value (value from the previous minute) for a particular emotion)
def two_difference_from_baseline(input_dataframe, baseline):
  '''Parameters:
  input_dataframe - dataframe returned from the previous function ("resample_and_compute_baseline")
  where emotions (mean) values are grouped by time,

  baseline: dataframe with baseline values for emotions returned from the previous function ("resample_and_compute_baseline"),


  ** function returns:
  dataframe with 2 new values for each emotion:
  1) "diff_from_baseline_": value computed as a measure of difference from the baseline value,
  2) "diff_from_previous_value_": value computed as a measure of difference from the previous value (i.e., value from the previous minute)
  '''
  data_resampled_all = input_dataframe.copy()

  data_resampled_all["diff_from_baseline_Arousal"] = data_resampled_all["Arousal"].apply(baseline_diff_avg, emotion = "Arousal", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Anger"] = data_resampled_all["Anger"].apply(baseline_diff_avg, emotion = "Anger", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Sadness"] = data_resampled_all["Sadness"].apply(baseline_diff_avg, emotion = "Sadness", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Disgust"] = data_resampled_all["Disgust"].apply(baseline_diff_avg, emotion = "Disgust", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Valence"] = data_resampled_all["Valence"].apply(baseline_diff_avg, emotion = "Valence", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Fear"] = data_resampled_all["Fear"].apply(baseline_diff_avg, emotion = "Fear", emotion_avg = baseline)
  data_resampled_all["diff_from_baseline_Happiness"] = data_resampled_all["Happiness"].apply(baseline_diff_avg, emotion = "Happiness", emotion_avg = baseline)

  difference_from_prev_value = data_resampled_all[['Arousal', 'Anger', 'Sadness', 'Disgust', 'Valence', 'Fear','Happiness']].diff().fillna(0).round(5)
  difference_from_prev_value.columns = ['diff_from_previous_value_Arousal', 'diff_from_previous_value_Anger',
                                        'diff_from_previous_value_Sadness', 'diff_from_previous_value_Disgust',
                                        'diff_from_previous_value_Valence', 'diff_from_previous_value_Fear',
                                        'diff_from_previous_value_Happiness']
  data_resampled_all = pd.merge(data_resampled_all, difference_from_prev_value, left_index=True, right_index=True, how="left")
  return data_resampled_all


def emotion_category(dataframe, emotive_words_column, affective_database_path, db_words = "Word", db_emotion_category = "Class"):
  '''Parameters:
  dataframe: dataframe with your data,

  content_lemmatized_column: str - name of a column in dataframe where lemmatized text is located,

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed,

  db_emotion_category: str - name of the column from affective database from where the categories will be taken
  '''

  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  affective_database = affective_database[[db_words, db_emotion_category]]
  affective_database.set_index(db_words, inplace=True)
  set_of_words = set(affective_database.index)

  all_emotion_categories = []
  for emotive_words in dataframe[emotive_words_column]:
    emotion_categories = [affective_database[db_emotion_category].loc[str(word)] if str(word) in set_of_words else np.nan for word in emotive_words]
    all_emotion_categories.append(emotion_categories)
  dataframe["Emotion_categories"] = all_emotion_categories
  return dataframe


def count_categories(dataframe, emotion_categories_column, affective_database_path, db_emotion_category):
  '''Parameters:
  dataframe: dataframe with data,

  emotion_categories_column: str - name of a column in dataframe where the list of emotion categories assigned to emotive words is located,

  affective_database_path: str - path to a file with affective database,

  db_emotion_category: str - name of the column from affective database from where the categories will be taken
  '''
  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  all_categories = affective_database[db_emotion_category].unique().tolist()

  dataframe["merge_indx"] = range(0, len(dataframe))
  from collections import Counter
  dataframe = pd.merge(dataframe, pd.DataFrame([Counter(x) for x in dataframe[emotion_categories_column]]).fillna(0).astype(int).add_prefix("CATEGORY_"), how='left', left_on="merge_indx", right_index=True)
  dataframe.drop(["merge_indx"], axis=1, inplace=True)

  for category in all_categories:
    if not "CATEGORY_"+category in dataframe.columns:
      dataframe["CATEGORY_"+category] = 0
  return dataframe




def get_polarity_score(dataframe, content_lemmatized_column, affective_database_path, db_words = "Word"):
  '''Parameters:
  dataframe: dataframe with your data,

  content_lemmatized_column: str - name of a column in dataframe where words-lemmas are listed

  affective_database_path: str - path to a file with affective database,

  db_words: str - name of a column in affective database where words are listed
  '''
  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  emotion_values = ["Polarity"]
  used_cols = [db_words] + emotion_values

  affective_database_polarity = affective_database[used_cols]
  affective_database_polarity.set_index(db_words, inplace=True)

  all_polarity_scores = []
  all_neg_scores_counts = []
  all_pos_scores_counts = []
  all_neg_scores = []
  all_pos_scores = []
  all_neg_percent = []
  all_pos_percent = []

  affective_database_polarity_words = affective_database[db_words].tolist()

  for lemmas_list in dataframe[content_lemmatized_column]:
    emotive_words = []
    for word in lemmas_list:
      if word in affective_database_polarity_words:
        emotive_words.append(word)
    
    if len(emotive_words) > 0:
      scores = affective_database_polarity.loc[emotive_words]

      polarity_score = (scores.sum()[0])
      #polarity_score = polarity_score / len(scores)
      all_polarity_scores.append(polarity_score)

      neg_scores_count = scores.where(scores["Polarity"] < 0).count()[0]
      all_neg_scores_counts.append(neg_scores_count)

      pos_scores_count = scores.where(scores["Polarity"] > 0).count()[0]
      all_pos_scores_counts.append(pos_scores_count)

      #absolute value easier to plot in one figure later with positive score
      neg_score = abs(np.sum(scores.where(scores["Polarity"] < 0))[0])
      #neg_score = neg_score / len(scores.where(scores["Polarity"] < 0))
      all_neg_scores.append(neg_score)

      pos_score = np.sum(scores.where(scores["Polarity"] > 0))[0]
      #pos_score = pos_score / len(scores.where(scores["Polarity"] > 0))
      all_pos_scores.append(pos_score)

      neg_percent = round((neg_scores_count / len(lemmas_list)), 3)
      all_neg_percent.append(neg_percent)

      pos_percent = round((pos_scores_count / len(lemmas_list)), 3)
      all_pos_percent.append(pos_percent)

    else:
      polarity_score=neg_scores_count=pos_scores_count=neg_score=pos_score=neg_percent=pos_percent = np.NaN
      all_polarity_scores.append(polarity_score)
      all_neg_scores_counts.append(neg_scores_count)
      all_pos_scores_counts.append(pos_scores_count)
      all_neg_scores.append(neg_score)
      all_pos_scores.append(pos_score)
      all_neg_percent.append(neg_percent)
      all_pos_percent.append(pos_percent)

  dataframe["Polarity_score"] = all_polarity_scores
  dataframe["Negative_polarity_count"] = all_neg_scores_counts
  dataframe["Positive_polarity_count"] = all_pos_scores_counts
  dataframe["Negative_polarity_score"] = all_neg_scores
  dataframe["Positive_polarity_score"] = all_pos_scores
  dataframe["Negative_percentage"] = all_neg_percent
  dataframe["Positive_percentage"] = all_pos_percent
  return dataframe


def get_valence_values(dataframe, content_lemmatized_column, affective_database_path, db_words = "Word"):
  '''Parameters: 
  dataframe: dataframe with your data,

  content_lemmatized_column: str - name of a column in dataframe where word-lemmas are listed, 
  
  affective_database_path: str - path to a file with affective database, 
  
  db_words: str - name of a column in affective database where words are listed, 

  * based on the implementation of Algorithm 1 in 
  [J. Kocoń, A. Janz, P. Miłkowski, K. M. Juszczyk, K. Klessa, M. Piasecki, M. Riegel, M. Wierzba, A. Marchewka, A. Czoska, D. Grimling, and B. Konat, 
  Recognition of emotions, valence and arousal in large-scale multi-domain text reviews,  
  In Proceedings of the 9th Language and Technology Conference. Human Language Technologies as a Challenge for Computer Science and Linguistics, 2019, p. 274–280]

  https://sentimenti.pl/wp-content/uploads/2021/05/LTC2019_Recognition_of_emotions__polarity_and_arousal_in_large_scale_multi_domain_text_reviews.pdf
  
  '''
  if affective_database_path.endswith(".xlsx"):
    affective_database = pd.read_excel(affective_database_path)
  elif affective_database_path.endswith(".csv"):
    affective_database = pd.read_csv(affective_database_path)

  emotion_values = ["Valence"]
  used_cols = [db_words] + emotion_values

  affective_database_valence = affective_database[used_cols]
  affective_database_valence.set_index(db_words, inplace=True)
  #valence_mean = affective_database_valence["Valence"].mean()
  affective_database_valence_words = affective_database[db_words].tolist()

  neg_valence_scores = []
  pos_valence_scores = []

  for lemmas_list in dataframe[content_lemmatized_column]:
    emotive_words = []
    for word in lemmas_list:
      if word in affective_database_valence_words:
        emotive_words.append(word)

    if len(emotive_words) > 0:
      scores = affective_database_valence.loc[emotive_words]
      neg_score = abs(np.sum(scores.where(scores["Valence"].round(1) < 0.5))[0])
      neg_score = (neg_score / len(emotive_words))
      neg_valence_scores.append(neg_score)
      pos_score = np.sum(scores.where(scores["Valence"].round(1) > 0.5))[0]
      pos_score = (pos_score / len(emotive_words))
      pos_valence_scores.append(pos_score)
    else:
      neg_score=pos_score = np.NaN 
      neg_valence_scores.append(neg_score)
      pos_valence_scores.append(pos_score)

  dataframe["Valence_negative"] = neg_valence_scores
  dataframe["Valence_positive"] = pos_valence_scores
  return dataframe



def universal_resample_polarity(input_dataframe, date_col, polarity_col, time_from, time_to, time_unit = "1 min"):
  '''Parameters:
  input_dataframe: your dataframe name with data,
  
  date_col: str - column name where datetime is,

  polarity_col: str - column name with polarity score you want to use, 
  
  time_from: str - starting time of the range you want to analyze in format "year-month-day hour:minutes:seconds", 
  e.g. '2019-10-08 19:30:00',
  
  time_to: str - ending time of the range you want to analyze in format "year-month-day hour:minutes:seconds", 
  e.g. '2019-10-08 19:30:00',
  
  time_unit: str - time unit for analysis
  *please follow instructions specified below to correctly set this parameter;  
  if you want time_unit to be number of  seconds, then specify number of seconds and use  sec  abbreviation, 
  
  if you want time_unit to be number of  minutes, then specify number of minutes and use  min  abbreviation, 
  
  if you want time_unit to be number of  hours, then specify number of hours and use  h  abbreviation, 
  
  if you want time_unit to be number of  days, then specify number of seconds and use  day  abbreviation, 
  
   --> *by default time_unit is set to 1 minute 

  ** function returns:
  firstly - dataframe with average values for polariy computed per time unit of specified time range,
  secondly - baseline values in a form of dataframe
  '''
  df = input_dataframe.copy()

  if "min" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("min", "T")
  elif "sec" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("sec", "S")
  elif "h" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("h", "H")
  elif "day" in time_unit:
    time_unit = time_unit.replace(" ", "").replace("day", "D")       

  emotions_cols = [polarity_col]
  columns_to_analyze = [date_col] + emotions_cols
  df = df[columns_to_analyze]
  df[date_col] = pd.to_datetime(df[date_col], dayfirst = True)
  df = df[(df[date_col] >= time_from) & (df[date_col] <= time_to)].sort_values(by = date_col)
  
  df = df.rename(columns={date_col:'Time'})
  df_described = df[emotions_cols].describe()
  df_described_mean = pd.DataFrame(df_described.loc["mean"]).T


  data_resampled = df.set_index("Time").resample(time_unit)[polarity_col].mean().fillna(0)

  data_resampled = data_resampled.reset_index()
  data_resampled["Date"] = data_resampled["Time"].dt.date
  data_resampled["Time"] = data_resampled["Time"].dt.time.apply(str)
  data_resampled.columns = ["Time", polarity_col+"_mean", "Date"]

  return data_resampled, df_described_mean



def polarity_diff_from_baseline(input_dataframe, polarity_col, baseline):
  '''Parameters:
  input_dataframe - dataframe returned from the previous function
  where polarity values are grouped by time,

  polarity_col: str - name of a column with polarity/other sentiment feature name, 

  baseline: dataframe with baseline values for emotions returned from the previous function ("resample_and_compute_baseline")
  '''
  data_resampled = input_dataframe.copy()

  data_resampled["diff_from_baseline_"+polarity_col] = data_resampled[polarity_col+"_mean"].apply(baseline_diff_avg, emotion = polarity_col, emotion_avg = baseline)
  difference_from_prev_value = data_resampled[[polarity_col+"_mean"]].diff().fillna(0).round(5)
  difference_from_prev_value.columns = ["diff_from_previous_value_"+polarity_col]
  data_resampled = pd.merge(data_resampled, difference_from_prev_value, left_index=True, right_index=True, how="left")
  return data_resampled




df = load_data("/content/drive/MyDrive/Colab Notebooks/twitter/processed_tweets/tweet_May_full_new.csv")

# example
df_valence_neg, base_neg = universal_resample_polarity(df, date_col = 'Data opublikownia wzmianki', polarity_col="Negative_percentage", 
                                     time_from = '2020-05-06 20:30:00', time_to = '2020-05-06 23:00:00', time_unit = "1 min")
df_valence_neg = polarity_diff_from_baseline(df_valence_neg, "Negative_percentage", base_neg)


df_valence_pos, base_pos = universal_resample_polarity(df, date_col = "Data opublikownia wzmianki", polarity_col="Positive_percentage", 
                                     time_from = '2020-05-06 20:30:00', time_to = '2020-05-06 23:00:00', time_unit = "1 min")
df_valence_pos = polarity_diff_from_baseline(df_valence_pos, "Positive_percentage", base_pos)


fig, ax1 = plt.subplots(1, 1, figsize=(12, 7))
x = list(df_valence_neg.Time)
ax1.plot(df_valence_neg["Time"], df_valence_neg["diff_from_baseline_Negative_percentage"]* 100, 
         label = "negative polarity %", color = "#840004", alpha = 0.85)
ax1.plot(df_valence_pos["Time"], df_valence_pos["diff_from_baseline_Positive_percentage"]* 100, 
         label = "positive polarity %", color = "#15B3AF", alpha = 0.8)
ax1.set_xticks(x[::5])
ax1.set_xticklabels(x[::5], rotation=90)
ax1.axhline(y = 0.0, color = "#5E5C5C", linestyle="--", label = "baseline", linewidth = 2, alpha = 0.8)
ax1.set_xlabel("\nTime")
ax1.set_title("Negativity and positivity in TVP 2020 May tweets\n\n", fontsize = 15)
ax1.set_ylabel("Value")
ax1.grid(axis="x")
plt.tight_layout()
plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.1), ncol=3)
plt.show()

